<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <title>Neuromodulatory Control of Reinforcement Learning Agents: A Computational Model of Dopaminergic and Serotonergic Influence</title>
    <style>
        body {
            font-family: "Computer Modern", Palatino, serif; /* Classic scientific paper font */
            line-height: 1.6;
            max-width: 850px;
            margin: 20px auto;
            padding: 20px;
            color: #1a1a1a;
            background-color: #fff;
        }
        h1, h2, h3 {
            color: #2c3e50;
            margin-top: 1.8em;
            margin-bottom: 0.8em;
            line-height: 1.2;
            border-bottom: 1px solid #eee;
            padding-bottom: 0.3em;
        }
        h1 { font-size: 2.2em; border-bottom: 2px solid #ccc; }
        h2 { font-size: 1.6em; }
        h3 { font-size: 1.3em; border-bottom: none; }
        p {
            margin: 1em 0;
            text-align: justify;
        }
        .abstract-section { /* Renamed for clarity */
            margin: 2em 0;
            padding: 1.5em;
            background: #f8f9fa;
            border-left: 5px solid #1a72a4; /* A more muted blue */
            /* font-style: italic; Removed for standard abstract formatting */
        }
        .abstract-section strong { /* Specifically for the "Abstract:" label */
            font-style: normal;
            font-weight: bold;
        }
        .keywords {
            margin-top: -1em; /* Keep this relative to abstract */
            margin-bottom: 2em;
            font-size: 0.9em;
            color: #555;
        }
        code, .variable {
            background: #f0f0f0;
            padding: 0.2em 0.4em;
            border-radius: 4px;
            font-family: 'Courier New', monospace;
            font-size: 0.95em;
        }
        pre {
            background: #f8f9fa;
            padding: 1em;
            border-radius: 4px;
            overflow-x: auto;
            border: 1px solid #eee;
        }
        figure {
            margin: 2em 0;
            text-align: center;
        }
        figcaption {
            font-style: italic;
            margin-top: 0.5em;
            color: #666;
            font-size: 0.9em;
        }
        .equation {
            margin: 1.5em 0;
            padding: 1em;
            text-align: center;
            overflow-x: auto;
        }
        .equation-label {
            float: right;
            color: #666;
            font-style: normal;
        }
        ul {
            margin: 1em 0;
            padding-left: 2em;
        }
        li {
            margin-bottom: 0.5em;
        }
        strong { /* General strong tag for emphasis */
            font-weight: bold;
        }
        em { /* General em tag for emphasis */
            font-style: italic;
        }
        .reference-list {
            margin-top: 2em;
            padding-left: 0;
            list-style-type: none;
        }
        .reference-list li {
            margin-bottom: 0.8em;
            padding-left: 1.5em;
            text-indent: -1.5em;
        }
    </style>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js" async></script>
</head>
<body>

    <h1>Neuromodulatory Control of Reinforcement Learning Agents: A Computational Model of Dopaminergic and Serotonergic Influence</h1>

    <div class="abstract-section">
        <p><strong>Abstract:</strong> Artificial intelligence systems often lack the nuanced behavioral flexibility observed in biological organisms, particularly in adapting to dynamic environments and shifting internal priorities. This work introduces a computational model that integrates analogs of two critical neuromodulators, dopamine (DA) and serotonin (5-HT), to dynamically influence the core mechanisms of a reinforcement learning (RL) agent. Phasic DA signaling, known to encode reward prediction errors (RPEs), is modeled herein as a direct modulator of the RPE signal's magnitude, thereby scaling its impact on value function updates and policy learning. Concurrently, 5-HT influence, associated with risk assessment, patience, and behavioral inhibition, is computationally realized through its effects on policy stochasticity (entropy) and sensitivity to predicted aversive outcomes. We demonstrate this framework within an actor-critic RL agent tasked with navigating a dynamic grid-world environment, simulating variations in DA and 5-HT levels. Our findings illustrate that these simulated neuromodulatory states give rise to distinct and predictable behavioral phenotypes, ranging from impulsive, reward-driven exploration under high DA conditions to cautious, risk-averse strategies under high 5-HT influence. This model offers a principled approach to imbuing RL agents with more sophisticated, context-dependent decision-making capabilities, paving the way for artificial agents that can more closely emulate the adaptive richness of biological intelligence.</p>
    </div>

    <p class="keywords">
        <strong>Keywords:</strong> Reinforcement Learning, Neuromodulation, Dopamine, Serotonin, Biologically Inspired AI, Actor-Critic, Computational Neuroscience, Adaptive Behavior, Risk Sensitivity.
    </p>

<h2>1. Introduction</h2>
    <p>
        The pursuit of artificial intelligence (AI) that emulates the sophisticated adaptability of biological organisms remains a central challenge. While contemporary reinforcement learning (RL) agents have achieved remarkable success in complex, well-defined tasks, they often exhibit behavioral rigidity when faced with dynamically changing environments or the need to balance conflicting internal goals (Sutton & Barto, 2018). Biological systems, in contrast, demonstrate an extraordinary capacity for fluid behavioral adjustments, a capability significantly shaped by neuromodulatory systems. These systems, through diffuse projection pathways, orchestrate global changes in brain state, profoundly influencing perception, learning, motivation, and action selection in response to both internal states and external contingencies (Dayan, 2012; Marder, 2012).
    </p>
    <p>
        Among the array of neuromodulators, dopamine (DA) and serotonin (5-HT) play pivotal roles in guiding learning and behavior. Phasic DA signals are robustly established as encoding reward prediction errors (RPEs)—the discrepancy between expected and received rewards—which serves as a crucial learning signal for reinforcing adaptive actions (Schultz et al., 1997). Beyond its role in learning, DA is also integral to motivated behavior, vigor, and the pursuit of goals. Serotonin, on the other hand, has a more multifaceted profile, implicated in the regulation of mood, patience, behavioral inhibition, and responses to aversive stimuli or uncertainty (Cools et al., 2011; Dayan & Huys, 2009). It is thought to play a key role in adjusting an organism's sensitivity to risk and in modulating the trade-off between immediate gratification and long-term outcomes.
    </p>
    <p>
        Inspired by these neurobiological functions, this paper addresses the research question: <em>How can computational analogs of dopamine and serotonin be integrated into reinforcement learning frameworks to endow agents with more biologically plausible and adaptive behavioral repertoires?</em> We propose a specific computational model where simulated DA and 5-HT levels dynamically influence core components of an actor-critic RL agent. We formalize DA's influence as a gain factor on the RPE signal, thereby directly modulating the efficacy of learning updates. Serotonin's influence is modeled by its impact on policy stochasticity (via an entropy term) and by adjusting the agent's sensitivity to potential risks or aversive outcomes within the environment.
    </p>
    <p>
        The primary contribution of this work is the development and analysis of this neuromodulated RL agent. We demonstrate, through simulations in a dynamic grid-world environment (referred to as "Pac-Mind"), how varying the levels of these simulated neuromodulators leads to qualitatively different and interpretable behavioral strategies—from highly explorative and reward-focused under high DA, to cautious and risk-averse under high 5-HT. This investigation aims to provide insights into how principles of neuromodulation can be harnessed to create AI systems capable of richer, more context-dependent decision-making, ultimately contributing to the development of more robust and intelligent artificial agents. Figure 1 (to be added) will provide a conceptual overview of the proposed neuromodulatory influences on the RL agent's learning loop.
    </p>

    <h2>2. Theoretical Foundations</h2>
    <p>
        Our model builds upon established principles in reinforcement learning and the neurobiology of dopamine and serotonin. This section outlines these foundational concepts.
    </p>

    <h3>2.1 Reinforcement Learning Framework</h3>
    <p>
        We consider an agent interacting with an environment over discrete time steps, formalized as a Markov Decision Process (MDP). An MDP is defined by a tuple <span class="variable">\( (S, A, P, R, \gamma) \)</span>, where <span class="variable">\(S\)</span> is the set of states, <span class="variable">\(A\)</span> is the set of actions, <span class="variable">\(P(s'|s,a)\)</span> is the state transition probability function, <span class="variable">\(R(s,a,s')\)</span> is the reward function, and <span class="variable">\(\gamma \in [0,1]\)</span> is the discount factor. The agent's goal is to learn a policy, <span class="variable">\(\pi(a|s)\)</span>, which is a mapping from states to probabilities of selecting each action, that maximizes the expected cumulative discounted reward.
    </p>
    <p>
        Value functions are central to many RL algorithms. The state-value function, <span class="variable">\(V^\pi(s)\)</span>, is the expected return starting from state <span class="variable">\(s\)</span> and following policy <span class="variable">\(\pi\)</span> thereafter. Similarly, the action-value function, <span class="variable">\(Q^\pi(s,a)\)</span>, is the expected return starting from state <span class="variable">\(s\)</span>, taking action <span class="variable">\(a\)</span>, and thereafter following policy <span class="variable">\(\pi\)</span>. These functions satisfy the Bellman equations, which provide a basis for iterative learning algorithms (Sutton & Barto, 2018).
    </p>
    <p>
        Our work specifically employs an Actor-Critic architecture. In this paradigm, the "actor" learns the policy <span class="variable">\(\pi(a|s; \theta)\)</span>, parameterized by <span class="variable">\(\theta\)</span>, while the "critic" learns a value function, typically <span class="variable">\(V(s; w)\)</span> or <span class="variable">\(Q(s,a; w)\)</span>, parameterized by <span class="variable">\(w\)</span>. The critic provides an evaluative signal that guides the actor's learning. A key quantity in actor-critic methods is the Temporal Difference (TD) error, calculated at time step <span class="variable">\(t\)</span> as:
    </p>
    <div class="equation">
        <span class="variable">\(\delta_t = R_{t+1} + \gamma V(S_{t+1}; w) - V(S_t; w)\)</span>
        <span class="equation-label">(1)</span>
    </div>
    <p>
        This TD error represents the discrepancy between the estimated value of the current state and a more informed estimate based on the received reward and the value of the next state. It serves as a crucial learning signal for updating both the actor and critic parameters.
    </p>

    <h3>2.2 Neurobiology of Dopamine and Reward Prediction Error</h3>
    <p>
        A cornerstone of modern reinforcement learning theory is its deep connection to the functioning of the brain's dopamine system. Seminal work by Schultz, Dayan, Montague, and colleagues (1997) demonstrated that the phasic firing of midbrain dopamine neurons does not simply signal reward itself, but rather encodes a reward prediction error (RPE). Specifically, these neurons exhibit a burst of activity when an unexpected reward occurs or when a reward is greater than anticipated (positive RPE). Conversely, they show a pause in firing if an expected reward is omitted or is smaller than anticipated (negative RPE). If a reward occurs exactly as predicted, there is no change in their baseline firing rate.
    </p>
    <p>
        This dopaminergic RPE signal is thought to be a primary mechanism for driving associative learning in the brain, particularly in structures like the basal ganglia. A positive RPE (DA burst) strengthens the neural pathways and synaptic connections that led to the better-than-expected outcome, making those actions more likely in the future. A negative RPE (DA dip) weakens the connections associated with the worse-than-expected outcome, reducing the likelihood of those actions. The mathematical formulation of the TD error (<span class="variable">\(\delta_t\)</span> in Equation 1) closely mirrors this biological RPE signal, providing a powerful bridge between computational RL and neuroscience. Figure 2 (to be added) will illustrate the characteristic firing patterns of dopamine neurons in response to varying reward predictions and outcomes.
    </p>

    <h3>2.3 Neurobiology of Serotonin, Risk, and Behavioral Control</h3>
    <p>
        The serotonin (5-HT) system is considerably more heterogeneous in its functions compared to dopamine, with projections innervating almost all areas of the brain. While not as directly tied to a single computational quantity like RPE, 5-HT is broadly implicated in the regulation of mood, emotional states, and complex behavioral adaptations (Cools et al., 2011; Lucki, 1998). In the context of decision-making and learning, 5-HT is often associated with behavioral inhibition, particularly in response to punishment or anticipated negative outcomes (Soubrié, 1986). For instance, altered 5-HT levels have been linked to changes in impulsivity and the willingness to wait for delayed rewards (Schweighofer et al., 2008).
    </p>
    <p>
        Furthermore, serotonin appears to play a crucial role in how organisms respond to uncertainty and risk. Studies suggest that 5-HT can modulate sensitivity to aversive events and influence the trade-off between exploration and exploitation (Dayan & Huys, 2009; Bari & Robbins, 2013). For example, increased serotonergic activity has been linked to increased harm avoidance and more cautious behavior in uncertain situations. This suggests that 5-HT may contribute to setting a general "tone" for how an agent approaches potentially hazardous or unpredictable elements of its environment, possibly by influencing the evaluation of negative future consequences or by promoting more varied (stochastic) behavioral patterns to avoid predictable negative outcomes. The precise computational formalisms for 5-HT's diverse roles are still an active area of research, but its influence on risk assessment and behavioral flexibility is a recurring theme.
    </p>




    <h2>3. Computational Model: Neuromodulated Actor-Critic Agent</h2>
    <p>
        Building upon the theoretical foundations, we now detail the architecture and learning rules of our neuromodulated reinforcement learning agent. The model integrates dopaminergic and serotonergic influences directly into the update mechanisms of a standard Actor-Critic framework.
    </p>

    <h3>3.1 Baseline Actor-Critic Agent</h3>
    <p>
        The core of our agent is an Actor-Critic model. The actor component is responsible for selecting actions and is parameterized by <span class="variable">\(\theta\)</span>. It learns a policy <span class="variable">\(\pi(A_t|S_t; \theta)\)</span>, which defines a probability distribution over actions <span class="variable">\(A_t\)</span> given the current state <span class="variable">\(S_t\)</span>. The critic component, parameterized by <span class="variable">\(w\)</span>, learns a state-value function <span class="variable">\(V(S_t; w)\)</span>, which estimates the expected future discounted reward from state <span class="variable">\(S_t\)</span>.
    </p>
    <p>
        At each time step <span class="variable">\(t\)</span>, the agent observes state <span class="variable">\(S_t\)</span>, selects an action <span class="variable">\(A_t \sim \pi(A_t|S_t; \theta)\)</span>, receives a reward <span class="variable">\(R_{t+1}\)</span>, and transitions to a new state <span class="variable">\(S_{t+1}\)</span>. The critic evaluates the TD error <span class="variable">\(\delta_t\)</span> as defined in Equation (1). This TD error is then used to update both the critic and actor parameters.
    </p>
    <p>
        The critic's parameters <span class="variable">\(w\)</span> are updated to minimize the TD error, typically via stochastic gradient descent:
    </p>
    <div class="equation">
        <span class="variable">\(w \leftarrow w + \alpha_c \cdot \delta_t \cdot \nabla_w V(S_t; w)\)</span>
        <span class="equation-label">(2)</span>
    </div>
    <p>
        where <span class="variable">\(\alpha_c\)</span> is the critic's learning rate. The actor's parameters <span class="variable">\(\theta\)</span> are updated to increase the probability of actions that lead to positive TD errors (or, more commonly, positive advantages <span class="variable">\(A_t = \delta_t\)</span> if a state-value critic is used). A common update rule is the policy gradient update:
    </p>
    <div class="equation">
        <span class="variable">\(\theta \leftarrow \theta + \alpha_a \cdot \nabla_\theta \log \pi(A_t|S_t; \theta) \cdot \delta_t\)</span>
        <span class="equation-label">(3)</span>
    </div>
    <p>
        where <span class="variable">\(\alpha_a\)</span> is the actor's learning rate.
    </p>

    <h3>3.2 Dopaminergic Modulation (Parameter <span class="variable">\(k_{DA}\)</span>)</h3>
    <p>
        The influence of dopamine is modeled as a scaling factor, <span class="variable">\(k_{DA}\)</span>, applied to the TD error <span class="variable">\(\delta_t\)</span>. This reflects the biological role of dopamine in modulating the strength of the RPE signal and its consequent impact on learning. The <span class="variable">\(k_{DA}\)</span> parameter represents the current "dopaminergic gain" or responsiveness of the system to prediction errors. A higher <span class="variable">\(k_{DA}\)</span> signifies an amplified response to RPEs, leading to more substantial updates to both actor and critic, while a lower <span class="variable">\(k_{DA}\)</span> dampens this learning signal.
    </p>
    <p>
        The modulated TD error, <span class="variable">\(\delta'_t\)</span>, is defined as:
    </p>
    <div class="equation">
        <span class="variable">\(\delta'_t = k_{DA} \cdot \delta_t\)</span>
        <span class="equation-label">(4)</span>
    </div>
    <p>
        This <span class="variable">\(\delta'_t\)</span> then replaces <span class="variable">\(\delta_t\)</span> in the update rules for both the critic (Equation 2) and the actor (Equation 3):
    </p>
    <div class="equation">
        <span class="variable">\(w \leftarrow w + \alpha_c \cdot \delta'_t \cdot \nabla_w V(S_t; w)\)</span>
        <span class="equation-label">(5)</span>
    </div>
    <div class="equation">
        <span class="variable">\(\theta \leftarrow \theta + \alpha_a \cdot \nabla_\theta \log \pi(A_t|S_t; \theta) \cdot \delta'_t\)</span>
        <span class="equation-label">(6)</span>
    </div>
    <p>
        For the purpose of this study, <span class="variable">\(k_{DA}\)</span> is treated as an external parameter that can be set to simulate different overall dopaminergic states (e.g., baseline, heightened, suppressed). Figure 3 (to be added) will depict how <span class="variable">\(k_{DA}\)</span> gates the flow of the RPE signal to the actor and critic update mechanisms.
    </p>

    <h3>3.3 Serotonergic Modulation (Parameters <span class="variable">\(k_{5HT\_entropy}\)</span> and <span class="variable">\(k_{5HT\_risk}\)</span>)</h3>
    <p>
        Serotonergic influence is modeled through two primary mechanisms reflecting its role in behavioral inhibition, risk sensitivity, and policy stochasticity.
    </p>
    <p>
        First, we introduce a policy entropy regularization term, scaled by <span class="variable">\(k_{5HT\_entropy}\)</span>, into the actor's objective function. The entropy of the policy, <span class="variable">\(H(\pi(\cdot|S_t; \theta))\)</span>, measures its randomness or exploratory nature. Maximizing policy entropy encourages the agent to explore more diverse actions and avoid premature convergence to suboptimal deterministic policies. A higher <span class="variable">\(k_{5HT\_entropy}\)</span> value thus promotes more stochastic and potentially cautious behavior, as the agent is less committed to a single action. The actor's objective function <span class="variable">\(J(\theta)\)</span> is augmented as follows:
    </p>
    <div class="equation">
        <span class="variable">\(J(\theta) \approx \mathbb{E}_{\pi} \left[ \sum_t \left( \log \pi(A_t|S_t; \theta) \cdot \delta'_t + k_{5HT\_entropy} \cdot H(\pi(\cdot|S_t; \theta)) \right) \right]\)</span>
        <span class="equation-label">(7)</span>
    </div>
    <p>
        This results in an additional term in the actor's gradient update (Equation 6), encouraging higher entropy when <span class="variable">\(k_{5HT\_entropy}\)</span> is positive.
    </p>
    <p>
        Second, to model risk sensitivity associated with serotonin, we introduce a risk term directly into the agent's perceived reward or value calculation, scaled by <span class="variable">\(k_{5HT\_risk}\)</span>. This term, <span class="variable">\(\text{Risk}(S_t, A_t)\)</span>, quantifies the anticipated danger or aversiveness associated with taking action <span class="variable">\(A_t\)</span> in state <span class="variable">\(S_t\)</span>, or with the resulting state <span class="variable">\(S_{t+1}\)</span>. For example, in an environment with hazards, <span class="variable">\(\text{Risk}(S_t, A_t)\)</span> could be a learned prediction of entering a penalty state or proximity to an aversive stimulus. The agent's learning updates effectively use a risk-adjusted TD error, where the definition of reward or value is modified. A simple way to conceptualize this is by modifying the immediate reward signal experienced by the agent before it forms the TD error:
    </p>
    <div class="equation">
        <span class="variable">\(R'_{t+1} = R_{t+1} - k_{5HT\_risk} \cdot \text{RiskSignal}(S_{t+1})\)</span>
        <span class="equation-label">(8)</span>
    </div>
    <p>
        where <span class="variable">\(\text{RiskSignal}(S_{t+1})\)</span> is a measure of the aversiveness of the resulting state. The TD error <span class="variable">\(\delta_t\)</span> (and subsequently <span class="variable">\(\delta'_t\)</span>) would then be calculated using <span class="variable">\(R'_{t+1}\)</span>. A higher <span class="variable">\(k_{5HT\_risk}\)</span> makes the agent more sensitive to these risk signals, effectively penalizing actions that lead to risky situations.
    </p>
    <p>
        Similar to <span class="variable">\(k_{DA}\)</span>, the parameters <span class="variable">\(k_{5HT\_entropy}\)</span> and <span class="variable">\(k_{5HT\_risk}\)</span> are controlled externally in this study to simulate varying serotonergic states. The interplay between these modulatory parameters allows for a rich spectrum of behavioral responses.
    </p>

    
</body>
</html>
