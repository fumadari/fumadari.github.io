<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <title>Neuromodulatory Control of Reinforcement Learning Agents: A Computational Model of Dopaminergic and Serotonergic Influence</title>
    <style>
        body {
            font-family: "Computer Modern", Palatino, serif; /* Classic scientific paper font */
            line-height: 1.6;
            max-width: 850px;
            margin: 20px auto;
            padding: 20px;
            color: #1a1a1a;
            background-color: #fff;
        }
        h1, h2, h3 {
            color: #2c3e50;
            margin-top: 1.8em;
            margin-bottom: 0.8em;
            line-height: 1.2;
            border-bottom: 1px solid #eee;
            padding-bottom: 0.3em;
        }
        h1 { font-size: 2.2em; border-bottom: 2px solid #ccc; }
        h2 { font-size: 1.6em; }
        h3 { font-size: 1.3em; border-bottom: none; }
        p {
            margin: 1em 0;
            text-align: justify;
        }
        .abstract-section { /* Renamed for clarity */
            margin: 2em 0;
            padding: 1.5em;
            background: #f8f9fa;
            border-left: 5px solid #1a72a4; /* A more muted blue */
            /* font-style: italic; Removed for standard abstract formatting */
        }
        .abstract-section strong { /* Specifically for the "Abstract:" label */
            font-style: normal;
            font-weight: bold;
        }
        .keywords {
            margin-top: -1em; /* Keep this relative to abstract */
            margin-bottom: 2em;
            font-size: 0.9em;
            color: #555;
        }
        code, .variable {
            background: #f0f0f0;
            padding: 0.2em 0.4em;
            border-radius: 4px;
            font-family: 'Courier New', monospace;
            font-size: 0.95em;
        }
        pre {
            background: #f8f9fa;
            padding: 1em;
            border-radius: 4px;
            overflow-x: auto;
            border: 1px solid #eee;
        }
        figure {
            margin: 2em 0;
            text-align: center;
        }
        figcaption {
            font-style: italic;
            margin-top: 0.5em;
            color: #666;
            font-size: 0.9em;
        }
        .equation {
            margin: 1.5em 0;
            padding: 1em;
            text-align: center;
            overflow-x: auto;
        }
        .equation-label {
            float: right;
            color: #666;
            font-style: normal;
        }
        ul {
            margin: 1em 0;
            padding-left: 2em;
        }
        li {
            margin-bottom: 0.5em;
        }
        strong { /* General strong tag for emphasis */
            font-weight: bold;
        }
        em { /* General em tag for emphasis */
            font-style: italic;
        }
        .reference-list {
            margin-top: 2em;
            padding-left: 0;
            list-style-type: none;
        }
        .reference-list li {
            margin-bottom: 0.8em;
            padding-left: 1.5em;
            text-indent: -1.5em;
        }
    </style>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js" async></script>
</head>
<body>

    <h1>Neuromodulatory Control of Reinforcement Learning Agents: A Computational Model of Dopaminergic and Serotonergic Influence</h1>

    <div class="abstract-section">
        <p><strong>Abstract:</strong> Artificial intelligence systems often lack the nuanced behavioral flexibility observed in biological organisms, particularly in adapting to dynamic environments and shifting internal priorities. This work introduces a computational model that integrates analogs of two critical neuromodulators, dopamine (DA) and serotonin (5-HT), to dynamically influence the core mechanisms of a reinforcement learning (RL) agent. Phasic DA signaling, known to encode reward prediction errors (RPEs), is modeled herein as a direct modulator of the RPE signal's magnitude, thereby scaling its impact on value function updates and policy learning. Concurrently, 5-HT influence, associated with risk assessment, patience, and behavioral inhibition, is computationally realized through its effects on policy stochasticity (entropy) and sensitivity to predicted aversive outcomes. We demonstrate this framework within an actor-critic RL agent tasked with navigating a dynamic grid-world environment, simulating variations in DA and 5-HT levels. Our findings illustrate that these simulated neuromodulatory states give rise to distinct and predictable behavioral phenotypes, ranging from impulsive, reward-driven exploration under high DA conditions to cautious, risk-averse strategies under high 5-HT influence. This model offers a principled approach to imbuing RL agents with more sophisticated, context-dependent decision-making capabilities, paving the way for artificial agents that can more closely emulate the adaptive richness of biological intelligence.</p>
    </div>

    <p class="keywords">
        <strong>Keywords:</strong> Reinforcement Learning, Neuromodulation, Dopamine, Serotonin, Biologically Inspired AI, Actor-Critic, Computational Neuroscience, Adaptive Behavior, Risk Sensitivity.
    </p>

<h2>1. Introduction</h2>
    <p>
        The pursuit of artificial intelligence (AI) that emulates the sophisticated adaptability of biological organisms remains a central challenge. While contemporary reinforcement learning (RL) agents have achieved remarkable success in complex, well-defined tasks, they often exhibit behavioral rigidity when faced with dynamically changing environments or the need to balance conflicting internal goals (Sutton & Barto, 2018). Biological systems, in contrast, demonstrate an extraordinary capacity for fluid behavioral adjustments, a capability significantly shaped by neuromodulatory systems. These systems, through diffuse projection pathways, orchestrate global changes in brain state, profoundly influencing perception, learning, motivation, and action selection in response to both internal states and external contingencies (Dayan, 2012; Marder, 2012).
    </p>
    <p>
        Among the array of neuromodulators, dopamine (DA) and serotonin (5-HT) play pivotal roles in guiding learning and behavior. Phasic DA signals are robustly established as encoding reward prediction errors (RPEs)—the discrepancy between expected and received rewards—which serves as a crucial learning signal for reinforcing adaptive actions (Schultz et al., 1997). Beyond its role in learning, DA is also integral to motivated behavior, vigor, and the pursuit of goals. Serotonin, on the other hand, has a more multifaceted profile, implicated in the regulation of mood, patience, behavioral inhibition, and responses to aversive stimuli or uncertainty (Cools et al., 2011; Dayan & Huys, 2009). It is thought to play a key role in adjusting an organism's sensitivity to risk and in modulating the trade-off between immediate gratification and long-term outcomes.
    </p>
    <p>
        Inspired by these neurobiological functions, this paper addresses the research question: <em>How can computational analogs of dopamine and serotonin be integrated into reinforcement learning frameworks to endow agents with more biologically plausible and adaptive behavioral repertoires?</em> We propose a specific computational model where simulated DA and 5-HT levels dynamically influence core components of an actor-critic RL agent. We formalize DA's influence as a gain factor on the RPE signal, thereby directly modulating the efficacy of learning updates. Serotonin's influence is modeled by its impact on policy stochasticity (via an entropy term) and by adjusting the agent's sensitivity to potential risks or aversive outcomes within the environment.
    </p>
    <p>
        The primary contribution of this work is the development and analysis of this neuromodulated RL agent. We demonstrate, through simulations in a dynamic grid-world environment (referred to as "Pac-Mind"), how varying the levels of these simulated neuromodulators leads to qualitatively different and interpretable behavioral strategies—from highly explorative and reward-focused under high DA, to cautious and risk-averse under high 5-HT. This investigation aims to provide insights into how principles of neuromodulation can be harnessed to create AI systems capable of richer, more context-dependent decision-making, ultimately contributing to the development of more robust and intelligent artificial agents. Figure 1 (to be added) will provide a conceptual overview of the proposed neuromodulatory influences on the RL agent's learning loop.
    </p>

    <h2>2. Theoretical Foundations</h2>
    <p>
        Our model builds upon established principles in reinforcement learning and the neurobiology of dopamine and serotonin. This section outlines these foundational concepts.
    </p>

    <h3>2.1 Reinforcement Learning Framework</h3>
    <p>
        We consider an agent interacting with an environment over discrete time steps, formalized as a Markov Decision Process (MDP). An MDP is defined by a tuple <span class="variable">\( (S, A, P, R, \gamma) \)</span>, where <span class="variable">\(S\)</span> is the set of states, <span class="variable">\(A\)</span> is the set of actions, <span class="variable">\(P(s'|s,a)\)</span> is the state transition probability function, <span class="variable">\(R(s,a,s')\)</span> is the reward function, and <span class="variable">\(\gamma \in [0,1]\)</span> is the discount factor. The agent's goal is to learn a policy, <span class="variable">\(\pi(a|s)\)</span>, which is a mapping from states to probabilities of selecting each action, that maximizes the expected cumulative discounted reward.
    </p>
    <p>
        Value functions are central to many RL algorithms. The state-value function, <span class="variable">\(V^\pi(s)\)</span>, is the expected return starting from state <span class="variable">\(s\)</span> and following policy <span class="variable">\(\pi\)</span> thereafter. Similarly, the action-value function, <span class="variable">\(Q^\pi(s,a)\)</span>, is the expected return starting from state <span class="variable">\(s\)</span>, taking action <span class="variable">\(a\)</span>, and thereafter following policy <span class="variable">\(\pi\)</span>. These functions satisfy the Bellman equations, which provide a basis for iterative learning algorithms (Sutton & Barto, 2018).
    </p>
    <p>
        Our work specifically employs an Actor-Critic architecture. In this paradigm, the "actor" learns the policy <span class="variable">\(\pi(a|s; \theta)\)</span>, parameterized by <span class="variable">\(\theta\)</span>, while the "critic" learns a value function, typically <span class="variable">\(V(s; w)\)</span> or <span class="variable">\(Q(s,a; w)\)</span>, parameterized by <span class="variable">\(w\)</span>. The critic provides an evaluative signal that guides the actor's learning. A key quantity in actor-critic methods is the Temporal Difference (TD) error, calculated at time step <span class="variable">\(t\)</span> as:
    </p>
    <div class="equation">
        <span class="variable">\(\delta_t = R_{t+1} + \gamma V(S_{t+1}; w) - V(S_t; w)\)</span>
        <span class="equation-label">(1)</span>
    </div>
    <p>
        This TD error represents the discrepancy between the estimated value of the current state and a more informed estimate based on the received reward and the value of the next state. It serves as a crucial learning signal for updating both the actor and critic parameters.
    </p>

    <h3>2.2 Neurobiology of Dopamine and Reward Prediction Error</h3>
    <p>
        A cornerstone of modern reinforcement learning theory is its deep connection to the functioning of the brain's dopamine system. Seminal work by Schultz, Dayan, Montague, and colleagues (1997) demonstrated that the phasic firing of midbrain dopamine neurons does not simply signal reward itself, but rather encodes a reward prediction error (RPE). Specifically, these neurons exhibit a burst of activity when an unexpected reward occurs or when a reward is greater than anticipated (positive RPE). Conversely, they show a pause in firing if an expected reward is omitted or is smaller than anticipated (negative RPE). If a reward occurs exactly as predicted, there is no change in their baseline firing rate.
    </p>
    <p>
        This dopaminergic RPE signal is thought to be a primary mechanism for driving associative learning in the brain, particularly in structures like the basal ganglia. A positive RPE (DA burst) strengthens the neural pathways and synaptic connections that led to the better-than-expected outcome, making those actions more likely in the future. A negative RPE (DA dip) weakens the connections associated with the worse-than-expected outcome, reducing the likelihood of those actions. The mathematical formulation of the TD error (<span class="variable">\(\delta_t\)</span> in Equation 1) closely mirrors this biological RPE signal, providing a powerful bridge between computational RL and neuroscience. Figure 2 (to be added) will illustrate the characteristic firing patterns of dopamine neurons in response to varying reward predictions and outcomes.
    </p>

    <h3>2.3 Neurobiology of Serotonin, Risk, and Behavioral Control</h3>
    <p>
        The serotonin (5-HT) system is considerably more heterogeneous in its functions compared to dopamine, with projections innervating almost all areas of the brain. While not as directly tied to a single computational quantity like RPE, 5-HT is broadly implicated in the regulation of mood, emotional states, and complex behavioral adaptations (Cools et al., 2011; Lucki, 1998). In the context of decision-making and learning, 5-HT is often associated with behavioral inhibition, particularly in response to punishment or anticipated negative outcomes (Soubrié, 1986). For instance, altered 5-HT levels have been linked to changes in impulsivity and the willingness to wait for delayed rewards (Schweighofer et al., 2008).
    </p>
    <p>
        Furthermore, serotonin appears to play a crucial role in how organisms respond to uncertainty and risk. Studies suggest that 5-HT can modulate sensitivity to aversive events and influence the trade-off between exploration and exploitation (Dayan & Huys, 2009; Bari & Robbins, 2013). For example, increased serotonergic activity has been linked to increased harm avoidance and more cautious behavior in uncertain situations. This suggests that 5-HT may contribute to setting a general "tone" for how an agent approaches potentially hazardous or unpredictable elements of its environment, possibly by influencing the evaluation of negative future consequences or by promoting more varied (stochastic) behavioral patterns to avoid predictable negative outcomes. The precise computational formalisms for 5-HT's diverse roles are still an active area of research, but its influence on risk assessment and behavioral flexibility is a recurring theme.
    </p>




    <h2>3. Computational Model: Neuromodulated Actor-Critic Agent</h2>
    <p>
        Building upon the theoretical foundations, we now detail the architecture and learning rules of our neuromodulated reinforcement learning agent. The model integrates dopaminergic and serotonergic influences directly into the update mechanisms of a standard Actor-Critic framework.
    </p>

    <h3>3.1 Baseline Actor-Critic Agent</h3>
    <p>
        The core of our agent is an Actor-Critic model. The actor component is responsible for selecting actions and is parameterized by <span class="variable">\(\theta\)</span>. It learns a policy <span class="variable">\(\pi(A_t|S_t; \theta)\)</span>, which defines a probability distribution over actions <span class="variable">\(A_t\)</span> given the current state <span class="variable">\(S_t\)</span>. The critic component, parameterized by <span class="variable">\(w\)</span>, learns a state-value function <span class="variable">\(V(S_t; w)\)</span>, which estimates the expected future discounted reward from state <span class="variable">\(S_t\)</span>.
    </p>
    <p>
        At each time step <span class="variable">\(t\)</span>, the agent observes state <span class="variable">\(S_t\)</span>, selects an action <span class="variable">\(A_t \sim \pi(A_t|S_t; \theta)\)</span>, receives a reward <span class="variable">\(R_{t+1}\)</span>, and transitions to a new state <span class="variable">\(S_{t+1}\)</span>. The critic evaluates the TD error <span class="variable">\(\delta_t\)</span> as defined in Equation (1). This TD error is then used to update both the critic and actor parameters.
    </p>
    <p>
        The critic's parameters <span class="variable">\(w\)</span> are updated to minimize the TD error, typically via stochastic gradient descent:
    </p>
    <div class="equation">
        <span class="variable">\(w \leftarrow w + \alpha_c \cdot \delta_t \cdot \nabla_w V(S_t; w)\)</span>
        <span class="equation-label">(2)</span>
    </div>
    <p>
        where <span class="variable">\(\alpha_c\)</span> is the critic's learning rate. The actor's parameters <span class="variable">\(\theta\)</span> are updated to increase the probability of actions that lead to positive TD errors (or, more commonly, positive advantages <span class="variable">\(A_t = \delta_t\)</span> if a state-value critic is used). A common update rule is the policy gradient update:
    </p>
    <div class="equation">
        <span class="variable">\(\theta \leftarrow \theta + \alpha_a \cdot \nabla_\theta \log \pi(A_t|S_t; \theta) \cdot \delta_t\)</span>
        <span class="equation-label">(3)</span>
    </div>
    <p>
        where <span class="variable">\(\alpha_a\)</span> is the actor's learning rate.
    </p>

    <h3>3.2 Dopaminergic Modulation (Parameter <span class="variable">\(k_{DA}\)</span>)</h3>
    <p>
        The influence of dopamine is modeled as a scaling factor, <span class="variable">\(k_{DA}\)</span>, applied to the TD error <span class="variable">\(\delta_t\)</span>. This reflects the biological role of dopamine in modulating the strength of the RPE signal and its consequent impact on learning. The <span class="variable">\(k_{DA}\)</span> parameter represents the current "dopaminergic gain" or responsiveness of the system to prediction errors. A higher <span class="variable">\(k_{DA}\)</span> signifies an amplified response to RPEs, leading to more substantial updates to both actor and critic, while a lower <span class="variable">\(k_{DA}\)</span> dampens this learning signal.
    </p>
    <p>
        The modulated TD error, <span class="variable">\(\delta'_t\)</span>, is defined as:
    </p>
    <div class="equation">
        <span class="variable">\(\delta'_t = k_{DA} \cdot \delta_t\)</span>
        <span class="equation-label">(4)</span>
    </div>
    <p>
        This <span class="variable">\(\delta'_t\)</span> then replaces <span class="variable">\(\delta_t\)</span> in the update rules for both the critic (Equation 2) and the actor (Equation 3):
    </p>
    <div class="equation">
        <span class="variable">\(w \leftarrow w + \alpha_c \cdot \delta'_t \cdot \nabla_w V(S_t; w)\)</span>
        <span class="equation-label">(5)</span>
    </div>
    <div class="equation">
        <span class="variable">\(\theta \leftarrow \theta + \alpha_a \cdot \nabla_\theta \log \pi(A_t|S_t; \theta) \cdot \delta'_t\)</span>
        <span class="equation-label">(6)</span>
    </div>
    <p>
        For the purpose of this study, <span class="variable">\(k_{DA}\)</span> is treated as an external parameter that can be set to simulate different overall dopaminergic states (e.g., baseline, heightened, suppressed). Figure 3 (to be added) will depict how <span class="variable">\(k_{DA}\)</span> gates the flow of the RPE signal to the actor and critic update mechanisms.
    </p>

    <h3>3.3 Serotonergic Modulation (Parameters <span class="variable">\(k_{5HT\_entropy}\)</span> and <span class="variable">\(k_{5HT\_risk}\)</span>)</h3>
    <p>
        Serotonergic influence is modeled through two primary mechanisms reflecting its role in behavioral inhibition, risk sensitivity, and policy stochasticity.
    </p>
    <p>
        First, we introduce a policy entropy regularization term, scaled by <span class="variable">\(k_{5HT\_entropy}\)</span>, into the actor's objective function. The entropy of the policy, <span class="variable">\(H(\pi(\cdot|S_t; \theta))\)</span>, measures its randomness or exploratory nature. Maximizing policy entropy encourages the agent to explore more diverse actions and avoid premature convergence to suboptimal deterministic policies. A higher <span class="variable">\(k_{5HT\_entropy}\)</span> value thus promotes more stochastic and potentially cautious behavior, as the agent is less committed to a single action. The actor's objective function <span class="variable">\(J(\theta)\)</span> is augmented as follows:
    </p>
    <div class="equation">
        <span class="variable">\(J(\theta) \approx \mathbb{E}_{\pi} \left[ \sum_t \left( \log \pi(A_t|S_t; \theta) \cdot \delta'_t + k_{5HT\_entropy} \cdot H(\pi(\cdot|S_t; \theta)) \right) \right]\)</span>
        <span class="equation-label">(7)</span>
    </div>
    <p>
        This results in an additional term in the actor's gradient update (Equation 6), encouraging higher entropy when <span class="variable">\(k_{5HT\_entropy}\)</span> is positive.
    </p>
    <p>
        Second, to model risk sensitivity associated with serotonin, we introduce a risk term directly into the agent's perceived reward or value calculation, scaled by <span class="variable">\(k_{5HT\_risk}\)</span>. This term, <span class="variable">\(\text{Risk}(S_t, A_t)\)</span>, quantifies the anticipated danger or aversiveness associated with taking action <span class="variable">\(A_t\)</span> in state <span class="variable">\(S_t\)</span>, or with the resulting state <span class="variable">\(S_{t+1}\)</span>. For example, in an environment with hazards, <span class="variable">\(\text{Risk}(S_t, A_t)\)</span> could be a learned prediction of entering a penalty state or proximity to an aversive stimulus. The agent's learning updates effectively use a risk-adjusted TD error, where the definition of reward or value is modified. A simple way to conceptualize this is by modifying the immediate reward signal experienced by the agent before it forms the TD error:
    </p>
    <div class="equation">
        <span class="variable">\(R'_{t+1} = R_{t+1} - k_{5HT\_risk} \cdot \text{RiskSignal}(S_{t+1})\)</span>
        <span class="equation-label">(8)</span>
    </div>
    <p>
        where <span class="variable">\(\text{RiskSignal}(S_{t+1})\)</span> is a measure of the aversiveness of the resulting state. The TD error <span class="variable">\(\delta_t\)</span> (and subsequently <span class="variable">\(\delta'_t\)</span>) would then be calculated using <span class="variable">\(R'_{t+1}\)</span>. A higher <span class="variable">\(k_{5HT\_risk}\)</span> makes the agent more sensitive to these risk signals, effectively penalizing actions that lead to risky situations.
    </p>
    <p>
        Similar to <span class="variable">\(k_{DA}\)</span>, the parameters <span class="variable">\(k_{5HT\_entropy}\)</span> and <span class="variable">\(k_{5HT\_risk}\)</span> are controlled externally in this study to simulate varying serotonergic states. The interplay between these modulatory parameters allows for a rich spectrum of behavioral responses.
    </p>




        <h2>4. Implementation and Simulation Environment: "Pac-Mind" Case Study</h2>
    <p>
        To investigate the effects of the proposed neuromodulatory controls, we implement the agent within a dynamic grid-world environment termed "Pac-Mind." This environment, inspired by the classic arcade game, provides a suitable testbed for evaluating an agent's ability to balance reward acquisition with risk avoidance under varying internal states.
    </p>

    <h3>4.1 Environment Details</h3>
    <p>
        The Pac-Mind environment consists of a <span class="variable">\(N \times M\)</span> grid (e.g., 20x20 cells) containing walls, collectible pellets, power pellets, and one or more adversarial "ghosts."
        <ul>
            <li><strong>Walls:</strong> Impassable cells that define the maze structure.</li>
            <li><strong>Pellets:</strong> Stationary items that, when collected by the agent, yield a small positive reward (e.g., <span class="variable">\(R_{pellet} = +1\)</span>) and are removed from the grid.</li>
            <li><strong>Power Pellets:</strong> Special stationary items. Collecting a power pellet yields a larger positive reward (e.g., <span class="variable">\(R_{power\_pellet} = +10\)</span>) and temporarily renders ghosts vulnerable. During this vulnerability period, contacting a ghost yields a significant positive reward (e.g., <span class="variable">\(R_{eat\_ghost} = +50\)</span>) and temporarily removes the ghost from play.</li>
            <li><strong>Ghosts:</strong> Adversarial entities that move within the maze. If a ghost occupies the same cell as the agent (and is not vulnerable), the agent incurs a large negative reward (e.g., <span class="variable">\(R_{collision} = -100\)</span>), and the episode typically terminates or the agent is reset. Ghost movement can be programmed with simple chase heuristics (e.g., moving towards the agent's current location) or learned through separate RL policies.</li>
        </ul>
        An episode concludes when the agent collides with a non-vulnerable ghost, all pellets are collected, or a maximum time step limit is reached. The agent's objective is to maximize its cumulative score. Figure 4 (to be added) will show a schematic of the Pac-Mind environment layout and its key components.
    </p>

    <h3>4.2 Agent Architecture and State Representation</h3>
    <p>
        The Pac-Mind agent employs the Actor-Critic architecture described in Section 3. To process the visual information from the grid world, the agent utilizes a Convolutional Neural Network (CNN) as its primary feature extractor.
    </p>
    <p>
        <strong>State Representation:</strong> The input to the agent at each time step <span class="variable">\(S_t\)</span> is a multi-channel image representing an egocentric window (e.g., an <span class="variable">\(11 \times 11\)</span> patch) centered on the agent's current position. Each channel in this input tensor encodes the presence of a specific entity within the window:
        <ul>
            <li>Channel 1: Wall locations.</li>
            <li>Channel 2: Pellet locations.</li>
            <li>Channel 3: Power pellet locations.</li>
            <li>Channel 4: Ghost locations (non-vulnerable).</li>
            <li>Channel 5: Vulnerable ghost locations.</li>
            <li>(Optional) Channel 6: Agent's own location (typically center of window).</li>
        </ul>
        This egocentric representation provides local contextual information necessary for decision-making.
    </p>
    <p>
        <strong>Network Architecture:</strong> The multi-channel input is fed into a CNN comprising typically two to three convolutional layers (e.g., with 16, 32, 64 filters, ReLU activations, and appropriate kernel sizes/strides), followed by a flattening operation. The output of the CNN serves as a shared feature vector. This feature vector is then passed to two separate fully connected output heads:
        <ul>
            <li><strong>Actor Head:</strong> A linear layer followed by a softmax activation function, outputting the probabilities <span class="variable">\(\pi(A_t|S_t; \theta)\)</span> for the four possible actions (Up, Down, Left, Right).</li>
            <li><strong>Critic Head:</strong> A linear layer outputting the scalar state value <span class="variable">\(V(S_t; w)\)</span>.</li>
        </ul>
        A diagram of this network architecture will be provided in Figure 5 (to be added).
    </p>

    <h3>4.3 Software and Simulation Parameters</h3>
    <p>
        The simulation environment and agent are implemented in Python. Neural network components and reinforcement learning algorithms leverage standard deep learning libraries such as PyTorch or TensorFlow. The environment itself is designed to be compatible with common RL interface standards, similar to OpenAI Gym (Brockman et al., 2016).
    </p>
    <p>
        Key simulation parameters include:
        <ul>
            <li>Learning rates: <span class="variable">\(\alpha_a\)</span> for the actor, <span class="variable">\(\alpha_c\)</span> for the critic (e.g., <span class="variable">\(10^{-4}\)</span> to <span class="variable">\(10^{-3}\)</span>).</li>
            <li>Discount factor: <span class="variable">\(\gamma\)</span> (e.g., 0.99).</li>
            <li>Neuromodulator coefficients: <span class="variable">\(k_{DA}\)</span>, <span class="variable">\(k_{5HT\_entropy}\)</span>, and <span class="variable">\(k_{5HT\_risk}\)</span> are varied across experimental conditions as described below.</li>
            <li>Training duration: Specified by a total number of training episodes or environment steps.</li>
            <li>Optimizer: Adam optimizer (Kingma & Ba, 2014) is commonly used for training the network parameters.</li>
        </ul>
    </p>
    <p>
        The <span class="variable">\(\text{RiskSignal}(S_{t+1})\)</span> for Equation (8) in the Pac-Mind context is defined based on proximity to non-vulnerable ghosts in the state <span class="variable">\(S_{t+1}\)</span>. For instance, it could be a binary indicator if <span class="variable">\(S_{t+1}\)</span> contains a ghost within a certain radius, or a continuous value inversely proportional to the distance to the nearest ghost.
    </p>

    <h3>4.4 Experimental Conditions</h3>
    <p>
        To analyze the impact of neuromodulation, experiments are conducted under several distinct conditions, each characterized by specific settings of the neuromodulatory parameters:
        <ul>
            <li><strong>Baseline Condition:</strong> Standard RL agent behavior with moderate, balanced settings (e.g., <span class="variable">\(k_{DA} = 1.0\)</span>, <span class="variable">\(k_{5HT\_entropy} = 0.01\)</span>, <span class="variable">\(k_{5HT\_risk} = 0.1\)</span>). These values are chosen to promote stable learning and reasonable performance.</li>
            <li><strong>High Dopamine (DA-Boost) Condition:</strong> Simulates a state of heightened reward sensitivity and motivation (e.g., <span class="variable">\(k_{DA} = 4.0\)</span>, with 5-HT parameters at baseline levels).</li>
            <li><strong>High Serotonin (5-HT-Boost / Cautious) Condition:</strong> Simulates a state of increased caution, risk aversion, and possibly higher behavioral variability (e.g., <span class="variable">\(k_{DA} = 1.0\)</span>, <span class="variable">\(k_{5HT\_entropy} = 0.05\)</span>, <span class="variable">\(k_{5HT\_risk} = 1.0\)</span>).</li>
            <li><strong>Combined States:</strong> Exploring interactions, such as High DA + High 5-HT, to observe how conflicting modulatory drives are resolved by the agent.</li>
            <li><strong>Control Condition:</strong> An agent trained without any explicit neuromodulatory scaling (equivalent to <span class="variable">\(k_{DA} = 1.0\)</span>, <span class="variable">\(k_{5HT\_entropy} = 0\)</span>, <span class="variable">\(k_{5HT\_risk} = 0\)</span>, though a small baseline entropy bonus is common practice in RL).</li>
        </ul>
        Each condition is typically run for multiple independent training seeds to account for the stochasticity inherent in RL training and allow for statistical comparison of results.
    </p>



<h2>5. Metrics and Evaluation</h2>
    <p>
        To comprehensively assess the impact of dopaminergic and serotonergic modulation on agent behavior and learning, we employ a suite of quantitative and qualitative metrics. These metrics are designed to capture not only overall task performance but also subtle shifts in strategy, risk preference, and exploration patterns.
    </p>

    <h3>5.1 Performance Metrics</h3>
    <p>
        These metrics evaluate the agent's effectiveness in achieving the primary objectives of the Pac-Mind task:
        <ul>
            <li><strong>Cumulative Score per Episode:</strong> The total reward accumulated by the agent in a single episode. Averaged over multiple episodes and training seeds, this is a primary indicator of overall task proficiency. Learning curves plotting average cumulative score against training episodes or steps will be crucial (see Figure 6a, to be added).</li>
            <li><strong>Episode Length / Survival Time:</strong> The number of time steps an agent survives before an episode terminates (e.g., due to collision with a ghost or task completion). Longer survival times, when coupled with high scores, indicate efficient and safe navigation.</li>
            <li><strong>Pellets Collected:</strong> The total number of regular pellets consumed per episode. This measures the agent's ability to systematically explore and exploit common reward sources.</li>
            <li><strong>Power Pellets Utilized:</strong> The number of power pellets collected. This indicates the agent's willingness to seek high-value, potentially strategic items.</li>
            <li><strong>Ghosts Eaten (during vulnerability):</strong> The number of ghosts consumed by the agent while they are vulnerable after a power pellet. This reflects opportunistic and potentially aggressive reward-seeking behavior.</li>
            <li><strong>Ghost Collisions (Non-vulnerable):</strong> The number of times the agent collides with a non-vulnerable ghost, resulting in a penalty. This is a direct measure of risk-taking failure or poor hazard avoidance.</li>
        </ul>
    </p>

    <h3>5.2 Behavioral Metrics</h3>
    <p>
        These metrics aim to characterize the agent's decision-making style and how it navigates the environment:
        <ul>
            <li><strong>Path Trajectory Analysis:</strong> Qualitative visualization of representative agent paths through the maze under different neuromodulatory conditions (see Figure 7, to be added). This can reveal patterns such as directness to goals, avoidance maneuvers, or hesitant movements.</li>
            <li><strong>State Visitation Frequency Heatmaps:</strong> Visualizations showing the frequency with which the agent visits different cells in the grid. These heatmaps can highlight preferred routes, areas of high exploration, or regions systematically avoided (see Figure 6b, to be added).</li>
            <li><strong>Policy Entropy (<span class="variable">\(H(\pi(\cdot|S_t; \theta))\)</span>):</strong> The average entropy of the agent's policy across states visited during an episode. Higher average entropy suggests more stochastic action selection and greater exploration, while lower entropy indicates more deterministic, exploitative behavior. This directly reflects the influence of the <span class="variable">\(k_{5HT\_entropy}\)</span> parameter.</li>
            <li><strong>Average Distance to Nearest Ghost:</strong> Calculated over time, this metric indicates the agent's typical proximity to threats. A consistently larger distance suggests more cautious, risk-averse behavior, correlating with <span class="variable">\(k_{5HT\_risk}\)</span>.</li>
            <li><strong>Action Selection Frequencies:</strong> Analysis of the distribution of chosen actions (Up, Down, Left, Right) in various contexts (e.g., near pellets, near ghosts) can reveal biases in movement patterns.</li>
            <li><strong>Decision Latency (if applicable in a more complex model):</strong> While not directly modeled here, in scenarios with explicit planning steps, the time taken to select an action could be a behavioral marker.</li>
        </ul>
    </p>

    <h3>5.3 Learning Process Metrics</h3>
    <p>
        These metrics provide insight into how neuromodulation affects the underlying learning dynamics of the agent:
        <ul>
            <li><strong>TD-Error (<span class="variable">\(\delta_t\)</span> and <span class="variable">\(\delta'_t\)</span>) Dynamics:</strong> Plotting the distribution, magnitude, and variance of TD errors over the course of training. The effect of <span class="variable">\(k_{DA}\)</span> on scaling <span class="variable">\(\delta'_t\)</span> should be observable. Large, frequent TD errors might indicate an unstable learning process or a highly volatile environment perception.</li>
            <li><strong>Value Function (<span class="variable">\(V(S_t; w)\)</span>) Evolution:</strong> Visualizing the learned state-value function as a heatmap across the maze at different stages of training and under different neuromodulatory conditions. This can show how the agent's assessment of state desirability changes (see Figure 6c, to be added). For instance, states near ghosts might have significantly lower values under high <span class="variable">\(k_{5HT\_risk}\)</span>.</li>
            <li><strong>Convergence Speed:</strong> The rate at which performance metrics (e.g., average score) asymptote during training. Different neuromodulatory settings might affect how quickly the agent reaches its peak performance or a stable policy.</li>
            <li><strong>Weight Norms of Actor/Critic Networks:</strong> Monitoring the magnitude of network weights can sometimes provide an indirect measure of learning stability or the complexity of the learned function.</li>
        </ul>
    </p>
    <p>
        By systematically collecting and analyzing these metrics across the experimental conditions outlined in Section 4.4, we aim to build a comprehensive understanding of how simulated dopamine and serotonin influence the agent's learning process, its emergent behavioral strategies, and its ultimate success in the Pac-Mind environment. Statistical significance tests will be applied where appropriate to compare outcomes across different conditions.
    </p>


        <h2>6. Expected Results and Analysis</h2>
    <p>
        Based on the established neurobiological roles of dopamine and serotonin and their proposed computational implementations, we anticipate distinct and measurable effects on agent behavior and learning across the different experimental conditions. This section outlines these expected outcomes and the planned analysis.
    </p>

    <h3>6.1 Baseline Condition</h3>
    <p>
        In the Baseline Condition (moderate <span class="variable">\(k_{DA}\)</span>, low-to-moderate <span class="variable">\(k_{5HT\_entropy}\)</span> and <span class="variable">\(k_{5HT\_risk}\)</span>), the agent is expected to learn a balanced strategy. It should effectively collect pellets, occasionally utilize power pellets, and demonstrate reasonable avoidance of non-vulnerable ghosts. Performance metrics, such as cumulative score, should show steady improvement over training episodes, eventually reaching a stable plateau (as depicted in a hypothetical Figure 6a). The learned value function (Figure 6c) should reflect higher values for states with pellets and lower values for states near immediate threats. Policy entropy will likely stabilize at a moderate level, indicating a balance between exploration and exploitation.
    </p>

    <h3>6.2 High Dopamine (DA-Boost) Condition</h3>
    <p>
        With an elevated <span class="variable">\(k_{DA}\)</span>, the RPE signal <span class="variable">\(\delta'_t\)</span> is amplified.
        <ul>
            <li><strong>Learning Dynamics:</strong> We expect faster initial learning, particularly from positive rewards. The agent might more rapidly associate actions with high-reward outcomes due to the magnified positive RPEs. However, amplified negative RPEs could also lead to more volatile value estimates or overly strong avoidance of even minor negative experiences if not balanced.
            </li>
            <li><strong>Behavioral Phenotype:</strong> The agent is predicted to exhibit more "reward-driven" or "impulsive" behavior. This may manifest as:
                <ul>
                    <li>More direct and rapid pursuit of pellets and especially power pellets (Figure 7a, to be added, showing more linear paths to rewards).</li>
                    <li>Increased willingness to take risks to obtain high-value rewards, potentially leading to more frequent ghost collisions despite faster learning. This trade-off will be critical to analyze via performance metrics.</li>
                    <li>A potential decrease in policy entropy if the agent quickly locks onto perceived optimal paths to rewards.</li>
                </ul>
            </li>
            <li><strong>Performance:</strong> The cumulative score might initially rise faster than baseline but could be capped or even reduced by increased penalties from risky behavior. The variance in scores across episodes might also be higher.
            </li>
        </ul>
    </p>

    <h3>6.3 High Serotonin (5-HT-Boost / Cautious) Condition</h3>
    <p>
        Elevated <span class="variable">\(k_{5HT\_entropy}\)</span> and <span class="variable">\(k_{5HT\_risk}\)</span> are expected to induce cautious, risk-averse behavior.
        <ul>
            <li><strong>Learning Dynamics:</strong> The <span class="variable">\(k_{5HT\_risk}\)</span> term, by effectively increasing the penalty for approaching hazards (Equation 8), should lead to a value function where states near ghosts are strongly devalued (Figure 6c, showing larger "valleys" around ghosts). The <span class="variable">\(k_{5HT\_entropy}\)</span> term will encourage more stochastic action selection.
            </li>
            <li><strong>Behavioral Phenotype:</strong> The agent is predicted to be more "cautious" or "anxious." This may manifest as:
                <ul>
                    <li>Slower, more meandering paths (Figure 7b, to be added), as the agent prioritizes safety and exploration over rapid reward acquisition.</li>
                    <li>Significantly increased average distance maintained from ghosts.</li>
                    <li>Reduced collection of pellets in high-risk areas, and potentially lower utilization of power pellets if they are situated near threats.</li>
                    <li>Higher average policy entropy.</li>
                </ul>
            </li>
            <li><strong>Performance:</strong> Cumulative scores are likely to be lower than baseline, particularly if caution prevents access to significant reward sources. However, the number of ghost collisions should be substantially reduced, leading to potentially longer episode survival times.
            </li>
        </ul>
    </p>

    <h3>6.4 Combined States and Interactions</h3>
    <p>
        Investigating conditions with combined high DA and high 5-HT will be particularly insightful. For instance, high <span class="variable">\(k_{DA}\)</span> alongside high <span class="variable">\(k_{5HT\_risk}\)</span> could create an internal conflict: a strong drive for reward coupled with a strong aversion to risk. This might lead to:
        <ul>
            <li>Hesitant or "dithering" behavior near rewards that are also close to threats.</li>
            <li>Highly specific strategies that attempt to secure rewards only when perceived risk is minimal.</li>
            <li>Potentially inefficient exploration as the agent oscillates between approach and avoidance.</li>
        </ul>
        The interaction between <span class="variable">\(k_{DA}\)</span>'s effect on learning speed/vigor and <span class="variable">\(k_{5HT\_entropy}\)</span>'s effect on policy stochasticity will also be examined.
    </p>

    <h3>6.5 Expected Metric Readouts Summary</h3>
    <p>
        The anticipated qualitative effects on key metrics across primary conditions are summarized in Table 1 (to be generated based on the text below if a table is desired, or kept as prose).
    </p>
    <p>
        In the <strong>DA-Boost</strong> condition, we expect to see higher pellet and power pellet collection rates, but also an increase in ghost collisions compared to baseline. Policy entropy might decrease as the agent focuses on direct reward paths. The TD-error magnitudes will be larger.
    </p>
    <p>
        In the <strong>5-HT-Boost</strong> condition, ghost collisions are expected to be minimal. Pellet collection might be slower, and average distance to ghosts significantly higher. Policy entropy should be elevated. The learned value function will show pronounced negative values around ghost-occupied regions.
    </p>
    <p>
        Analysis will involve plotting learning curves for performance metrics, visualizing behavioral patterns (paths, heatmaps), and comparing statistical distributions of behavioral and learning process metrics across conditions. For instance, box plots comparing cumulative scores, episode lengths, and ghost collisions across the different neuromodulatory states (as might be shown in a Figure 8, to be added) will be used to highlight significant differences. The correlation between the set neuromodulatory parameters and the observed metrics will be central to validating the model's intended functionality.
    </p>


    <h2>7. Discussion</h2>
    <p>
        The proposed computational model aims to bridge concepts from neuroscience and artificial intelligence by demonstrating how analogs of dopamine and serotonin can introduce nuanced, adaptive control into reinforcement learning agents. The anticipated results, as outlined in the previous section, suggest that this framework can successfully generate distinct behavioral phenotypes, moving beyond monolithic agent strategies towards more flexible and context-dependent decision-making.
    </p>
    <p>
        A key strength of this approach lies in its grounding in established neurobiological functions. The modeling of dopamine's influence as a gain on the reward prediction error (RPE) directly reflects its well-documented role in scaling learning signals (Schultz et al., 1997). Similarly, linking serotonin to policy stochasticity via entropy and to risk aversion via direct penalty modulation aligns with its complex but recognized involvement in behavioral inhibition, patience, and responses to aversive stimuli (Dayan & Huys, 2009; Cools et al., 2011). By manipulating a small set of parameters (<span class="variable">\(k_{DA}\)</span>, <span class="variable">\(k_{5HT\_entropy}\)</span>, <span class="variable">\(k_{5HT\_risk}\)</span>), we expect to observe emergent behaviors that are qualitatively similar to those seen in biological organisms under varying internal states or pharmacological manipulations affecting these neuromodulatory systems. For example, the "DA-Boost" condition is anticipated to produce an agent that is more akin to an impulsive, reward-seeking individual, while the "5-HT-Boost" condition may yield an agent that behaves cautiously, reminiscent of heightened anxiety or risk sensitivity.
    </p>
    <p>
        The implications for AI design are significant. Current RL agents often require extensive retraining or complex architectural changes to adapt their core behavioral strategies, such as their exploration-exploitation balance or risk posture. The neuromodulatory framework presented here offers a more dynamic mechanism for such adaptations. By adjusting a few global parameters, an agent could potentially shift its behavior significantly without altering its underlying learned knowledge (the weights of its neural networks). This could be particularly valuable in non-stationary environments where optimal strategies change over time, or in applications requiring agents to adopt different "personas" or operational modes. For instance, a robotic agent might operate in a high-DA mode for rapid exploration of a new, safe environment, but switch to a high-5-HT mode when navigating a hazardous area or when conserving energy becomes paramount. This dynamic control over an agent's "computational mood" could lead to more robust and versatile AI systems.
    </p>
    <p>
        However, it is crucial to acknowledge the simplifications inherent in this model. Real neuromodulatory systems are vastly more complex, involving intricate interactions between multiple neurotransmitters, diverse receptor subtypes, and brain-wide circuits. Our model reduces these complexities to a few key parameters and functional effects. For instance, the distinction between tonic (baseline) and phasic (event-driven) components of DA and 5-HT signaling is not explicitly captured, though <span class="variable">\(k_{DA}\)</span> could be interpreted as modulating the impact of phasic RPE signals. Furthermore, the external, manual setting of <span class="variable">\(k_{DA}\)</span> and <span class="variable">\(k_{5HT}\)</span> parameters is a simplification; in biological systems, the levels and effects of neuromodulators are themselves regulated by complex feedback loops and internal state assessments.
    </p>
    <p>
        Future work should aim to address some of these limitations. A significant step would be to develop mechanisms for endogenous regulation of these neuromodulatory parameters. For example, an agent could learn to adjust its own <span class="variable">\(k_{DA}\)</span> or <span class="variable">\(k_{5HT}\)</span> levels based on sustained performance metrics, perceived environmental volatility, or internal "needs" (e.g., low energy reserves in a robot might trigger a more cautious, 5-HT-like state). This would constitute a form of meta-learning, where the agent learns how to modulate its own learning and decision-making processes. Exploring the computational roles of other neuromodulators, such as norepinephrine (linked to arousal and task engagement) or acetylcholine (implicated in attention and memory), and their interactions, presents another rich avenue for research. Applying and validating this framework on more complex, continuous-control tasks and physical robotic platforms will also be essential for assessing its real-world utility. Finally, a deeper investigation into the interplay between these global modulatory signals and local synaptic plasticity rules could yield further insights into the construction of truly adaptive intelligent systems.
    </p>

    
</body>
</html>
