<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <title>Neuromodulatory Control of Reinforcement Learning Agents: A Computational Model of Dopaminergic and Serotonergic Influence</title>
    <style>
        body {
            font-family: "Computer Modern", Palatino, serif; /* Classic scientific paper font */
            line-height: 1.6;
            max-width: 850px;
            margin: 20px auto;
            padding: 20px;
            color: #1a1a1a;
            background-color: #fff;
        }
        h1, h2, h3 {
            color: #2c3e50;
            margin-top: 1.8em;
            margin-bottom: 0.8em;
            line-height: 1.2;
            border-bottom: 1px solid #eee;
            padding-bottom: 0.3em;
        }
        h1 { font-size: 2.2em; border-bottom: 2px solid #ccc; }
        h2 { font-size: 1.6em; }
        h3 { font-size: 1.3em; border-bottom: none; }
        p {
            margin: 1em 0;
            text-align: justify;
        }
        .abstract-section { /* Renamed for clarity */
            margin: 2em 0;
            padding: 1.5em;
            background: #f8f9fa;
            border-left: 5px solid #1a72a4; /* A more muted blue */
            /* font-style: italic; Removed for standard abstract formatting */
        }
        .abstract-section strong { /* Specifically for the "Abstract:" label */
            font-style: normal;
            font-weight: bold;
        }
        .keywords {
            margin-top: -1em; /* Keep this relative to abstract */
            margin-bottom: 2em;
            font-size: 0.9em;
            color: #555;
        }
        code, .variable {
            background: #f0f0f0;
            padding: 0.2em 0.4em;
            border-radius: 4px;
            font-family: 'Courier New', monospace;
            font-size: 0.95em;
        }
        pre {
            background: #f8f9fa;
            padding: 1em;
            border-radius: 4px;
            overflow-x: auto;
            border: 1px solid #eee;
        }
        figure {
            margin: 2em 0;
            text-align: center;
        }
        figcaption {
            font-style: italic;
            margin-top: 0.5em;
            color: #666;
            font-size: 0.9em;
        }
        .equation {
            margin: 1.5em 0;
            padding: 1em;
            text-align: center;
            overflow-x: auto;
        }
        .equation-label {
            float: right;
            color: #666;
            font-style: normal;
        }
        ul {
            margin: 1em 0;
            padding-left: 2em;
        }
        li {
            margin-bottom: 0.5em;
        }
        strong { /* General strong tag for emphasis */
            font-weight: bold;
        }
        em { /* General em tag for emphasis */
            font-style: italic;
        }
        .reference-list {
            margin-top: 2em;
            padding-left: 0;
            list-style-type: none;
        }
        .reference-list li {
            margin-bottom: 0.8em;
            padding-left: 1.5em;
            text-indent: -1.5em;
        }
    </style>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js" async></script>
</head>
<body>

    <h1>Neuromodulatory Control of Reinforcement Learning Agents: A Computational Model of Dopaminergic and Serotonergic Influence</h1>

    <div class="abstract-section">
        <p><strong>Abstract:</strong> Artificial intelligence systems often lack the nuanced behavioral flexibility observed in biological organisms, particularly in adapting to dynamic environments and shifting internal priorities. This work introduces a computational model that integrates analogs of two critical neuromodulators, dopamine (DA) and serotonin (5-HT), to dynamically influence the core mechanisms of a reinforcement learning (RL) agent. Phasic DA signaling, known to encode reward prediction errors (RPEs), is modeled herein as a direct modulator of the RPE signal's magnitude, thereby scaling its impact on value function updates and policy learning. Concurrently, 5-HT influence, associated with risk assessment, patience, and behavioral inhibition, is computationally realized through its effects on policy stochasticity (entropy) and sensitivity to predicted aversive outcomes. We demonstrate this framework within an actor-critic RL agent tasked with navigating a dynamic grid-world environment, simulating variations in DA and 5-HT levels. Our findings illustrate that these simulated neuromodulatory states give rise to distinct and predictable behavioral phenotypes, ranging from impulsive, reward-driven exploration under high DA conditions to cautious, risk-averse strategies under high 5-HT influence. This model offers a principled approach to imbuing RL agents with more sophisticated, context-dependent decision-making capabilities, paving the way for artificial agents that can more closely emulate the adaptive richness of biological intelligence.</p>
    </div>

    <p class="keywords">
        <strong>Keywords:</strong> Reinforcement Learning, Neuromodulation, Dopamine, Serotonin, Biologically Inspired AI, Actor-Critic, Computational Neuroscience, Adaptive Behavior, Risk Sensitivity.
    </p>

<h2>1. Introduction</h2>
    <p>
        The pursuit of artificial intelligence (AI) that emulates the sophisticated adaptability of biological organisms remains a central challenge. While contemporary reinforcement learning (RL) agents have achieved remarkable success in complex, well-defined tasks, they often exhibit behavioral rigidity when faced with dynamically changing environments or the need to balance conflicting internal goals (Sutton & Barto, 2018). Biological systems, in contrast, demonstrate an extraordinary capacity for fluid behavioral adjustments, a capability significantly shaped by neuromodulatory systems. These systems, through diffuse projection pathways, orchestrate global changes in brain state, profoundly influencing perception, learning, motivation, and action selection in response to both internal states and external contingencies (Dayan, 2012; Marder, 2012).
    </p>
    <p>
        Among the array of neuromodulators, dopamine (DA) and serotonin (5-HT) play particularly pivotal roles in guiding learning and behavior. Phasic DA signals are robustly established as encoding reward prediction errors (RPEs)—the discrepancy between expected and received rewards—which serves as a crucial learning signal for reinforcing adaptive actions (Schultz et al., 1997). Beyond its role in learning, DA is also integral to motivated behavior, vigor, and the pursuit of goals. Serotonin, on the other hand, has a more multifaceted profile, implicated in the regulation of mood, patience, behavioral inhibition, and responses to aversive stimuli or uncertainty (Cools et al., 2011; Dayan & Huys, 2009). It is thought to play a key role in adjusting an organism's sensitivity to risk and in modulating the trade-off between immediate gratification and long-term outcomes.
    </p>
    <p>
        Inspired by these neurobiological functions, this paper addresses the research question: <em>How can computational analogs of dopamine and serotonin be integrated into reinforcement learning frameworks to endow agents with more biologically plausible and adaptive behavioral repertoires?</em> We propose a specific computational model where simulated DA and 5-HT levels dynamically influence core components of an actor-critic RL agent. We formalize DA's influence as a gain factor on the RPE signal, thereby directly modulating the efficacy of learning updates. Serotonin's influence is modeled by its impact on policy stochasticity (via an entropy term) and by adjusting the agent's sensitivity to potential risks or aversive outcomes within the environment.
    </p>
    <p>
        The primary contribution of this work is the development and analysis of this neuromodulated RL agent. We demonstrate, through simulations in a dynamic grid-world environment (referred to as "Pac-Mind"), how varying the levels of these simulated neuromodulators leads to qualitatively different and interpretable behavioral strategies—from highly explorative and reward-focused under high DA, to cautious and risk-averse under high 5-HT. This investigation aims to provide insights into how principles of neuromodulation can be harnessed to create AI systems capable of richer, more context-dependent decision-making, ultimately contributing to the development of more robust and intelligent artificial agents. Figure 1 (to be added) will provide a conceptual overview of the proposed neuromodulatory influences on the RL agent's learning loop.
    </p>

</body>
</html>


    
</body>
</html>
