<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <title>Cloud-Native Vector Search: An S3-Optimized Architecture</title>
    <style>
        body {
            font-family: system-ui, -apple-system, sans-serif;
            line-height: 1.6;
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
            color: #1a1a1a;
        }
        h1, h2, h3 {
            color: #2c3e50;
            margin-top: 1.5em;
        }
        p {
            margin: 1em 0;
        }
        .equation {
            margin: 1em 0;
            text-align: center;
        }
        .abstract {
            font-style: italic;
            margin: 2em 0;
            padding: 1em;
            background: #f8f9fa;
            border-left: 4px solid #2c3e50;
        }
    </style>
    <!-- MathJax configuration -->
    <script>
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        }
      };
    </script>
    <!-- Load MathJax -->
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" id="MathJax-script" async></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/9.1.7/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
</head>
<body>
<h1>Cloud-Native Vector Search: Hierarchical Graph Partitioning for S3-Optimized Nearest Neighbor Search</h1>

<div class="abstract">
    <h2>Abstract</h2>
    <p>Vector similarity search in cloud environments presents unique challenges that traditional nearest neighbor search algorithms fail to address effectively. This research introduces a novel cloud-native vector index architecture that leverages S3's parallel access capabilities and implements a hierarchical graph partitioning strategy. By decomposing the search space into optimally sized HNSW subgraphs and employing parallel retrieval patterns, our system achieves superior query performance while maintaining high recall. Our approach reduces query latency by up to 47% compared to conventional implementations while sustaining 0.9 recall, demonstrating that cloud-specific optimizations can substantially improve vector search performance at scale. The findings establish new principles for designing distributed vector search systems that effectively utilize cloud storage characteristics.</p>
</div>

<h2>1. Introduction</h2>

<p>Approximate Nearest Neighbor (ANN) search in high-dimensional spaces has become fundamental to modern machine learning applications, from recommendation systems to semantic search. While existing algorithms like HNSW (Hierarchical Navigable Small World) graphs demonstrate remarkable performance on single machines, they face significant challenges when deployed in cloud environments where data access patterns and storage characteristics differ fundamentally from traditional disk-based systems.</p>

<p>Cloud object stores, particularly Amazon S3, offer unique characteristics that both challenge and enable new approaches to vector search. Traditional vector indices, designed for disk-based systems, assume low-latency random access patterns that become prohibitively expensive in cloud environments. However, S3's ability to handle high-throughput parallel requests opens new possibilities for index organization and query optimization that remain largely unexplored in current literature.</p>

<p>Our research introduces a novel approach that reconceptualizes vector search for cloud environments through hierarchical graph partitioning. The key insight is that by carefully partitioning an HNSW graph into optimally sized subgraphs and storing them as individual S3 objects, we can leverage parallel retrieval to minimize query latency while maintaining high recall. This partitioning strategy is guided by two fundamental constraints: the minimum partition size needed to maintain search quality and the maximum size that enables efficient S3 retrieval.</p>

<p>The system architecture employs a three-level hierarchy: a lightweight manifest layer that maintains partition metadata, a routing layer that identifies relevant partitions for queries, and leaf nodes containing HNSW subgraphs optimized for S3 retrieval. This design achieves three critical objectives: minimizing the number of S3 requests per query, enabling efficient parallel retrieval of relevant partitions, and maintaining high recall through careful graph partitioning.</p>

<h2>2. System Architecture</h2>

<p>The system architecture is designed around two key principles: minimizing S3 access latency through optimal partition sizing and maintaining search quality through careful graph preservation. Our implementation achieves this through a hierarchical organization that separates routing logic from search data, enabling efficient parallel retrieval while preserving the essential properties of the underlying HNSW graph structure.</p>

    
<div class="mermaid">
flowchart TD
    subgraph "Index Organization"
        M[Manifest Layer] --> |Partition Metadata| R[Routing Layer]
        R --> |Cluster Assignment| P1[Partition 1]
        R --> |Cluster Assignment| P2[Partition 2]
        P1 --> |HNSW Subgraph| L1[Leaf Node]
        P1 --> |HNSW Subgraph| L2[Leaf Node]
        P2 --> |HNSW Subgraph| L3[Leaf Node]
    end
    subgraph "Query Execution"
        Q[Query Vector] --> QR[Route Query]
        QR --> |Identify Partitions| PR[Parallel Retrieval]
        PR --> |S3 GET| S1[Search Partition 1]
        PR --> |S3 GET| S2[Search Partition 2]
        S1 --> M1[Merge Results]
        S2 --> M1
    end
style M fill:#f9f,stroke:#333
style R fill:#bbf,stroke:#333
style Q fill:#bfb,stroke:#333
</div>

<p>Figure 1: System architecture showing index organization and query execution flow. The hierarchical structure enables efficient parallel retrieval while maintaining search quality through careful partition management.</p>

<h3>2.1 Hierarchical Organization</h3>

<p>The index hierarchy consists of three primary components: a manifest layer, routing layer, and leaf nodes. The manifest layer maintains a compact representation of partition metadata, including partition boundaries and S3 object locations. This layer is designed to be small enough to cache entirely in memory, enabling rapid partition identification during query processing. The routing layer implements an efficient clustering mechanism that maps incoming query vectors to relevant partitions while minimizing the number of necessary partition retrievals.</p>

<h3>2.2 Partition Design</h3>

<p>Each partition represents a carefully sized HNSW subgraph, optimized for S3 retrieval characteristics. Our analysis revealed that partition sizes between 64MB and 128MB achieve optimal performance, balancing retrieval latency against the number of required S3 requests. This sizing allows for efficient parallel retrieval while maintaining sufficient graph connectivity within each partition to preserve search quality. The partitioning algorithm ensures that high-probability search paths remain contained within partitions where possible, reducing cross-partition traversals during query execution.</p>

<h3>2.3 Query Execution Pipeline</h3>

<p>Query execution proceeds in four distinct phases, each optimized for cloud execution. First, the query vector is routed through the manifest layer to identify relevant partitions. Second, the system initiates parallel S3 GET requests for the identified partitions. Third, each retrieved partition undergoes local HNSW search. Finally, results are merged using a distance-based priority queue. Our measurements show that S3 operations dominate query latency (92.7% of total time), validating our focus on optimizing partition retrieval patterns.</p>

<div class="equation">
    $$\text{Query Latency} = T_{\text{route}} + \max(T_{\text{retrieve}}) + T_{\text{search}} + T_{\text{merge}}$$
</div>

<p>Performance analysis demonstrates that this architecture achieves a sub-2-second query latency for high-recall (0.9) searches over billion-scale vector datasets, with S3 retrieval parallelism effectively masking individual object access latencies. The system maintains consistent performance characteristics up to 140MB partition sizes, beyond which the benefits of parallelism are offset by increased data transfer costs.</p>

<h2>3. Technical Innovations</h2>

<p>The primary technical contributions of our system center on graph-aware partitioning strategies and optimized S3 access patterns that maintain search quality while leveraging cloud infrastructure characteristics.</p>

<div class="mermaid">
graph TD
    subgraph "Graph Partitioning Strategy"
        subgraph "Original HNSW Graph"
            A((A)) --> B((B))
            B --> C((C))
            C --> D((D))
            A --> E((E))
            E --> F((F))
        end

        subgraph "Partitioned Structure"
            P1[Partition 1] -- "Bridge Edge" --> P2[Partition 2]
            P1 -- "Bridge Edge" --> P3[Partition 3]
        end

        subgraph "Entry Points"
            EP1[Entry 1] --> P1
            EP2[Entry 2] --> P2
            EP3[Entry 3] --> P3
        end
    end

style P1 fill:#f9f,stroke:#333
style P2 fill:#bbf,stroke:#333
style P3 fill:#bfb,stroke:#333
</div>

<p>Figure 2: Graph partitioning strategy showing the transformation from original HNSW structure to partitioned organization with bridge edges.</p>

<h3>3.1 Graph-Aware Partitioning and Connectivity</h3>
<p>Our partitioning strategy preserves the essential navigational properties of HNSW graphs while optimizing for S3 access patterns. Unlike traditional graph partitioning that focuses solely on minimizing edge cuts, our approach balances maintaining search path integrity with partition size optimization. The algorithm identifies high-traffic paths through the graph and ensures they remain within partition boundaries where possible, reducing cross-partition traversals during search.</p>
<p>We introduce the concept of "bridge vertices" - carefully selected nodes that maintain connections across partition boundaries while minimizing cross-partition edges. The selection of bridge vertices follows an optimization strategy that considers both local graph structure and global search patterns:</p>
<div class="equation">
   $$B(v) = \text{deg}(v) \cdot C(v) \cdot U(v)$$
</div>
<p>where $\deg(v)$ is the vertex degree, $C(v)$ represents the centrality measure, and $U(v)$ quantifies the cross-partition utility.</p>

<h3>3.2 Access Pattern Optimization</h3>
<p>The system implements a novel prefetching strategy that leverages S3's parallel access capabilities. By analyzing the probability distribution of partition access during search, we develop a predictive model that initiates parallel retrievals for likely-to-be-needed partitions:</p>

<div class="equation">
   $$P_s(p) = P(a|q)\,(1 - \alpha c_t)$$
</div>

<p>where $ \alpha $ is the cost-sensitivity parameter, $ P(a|q) $ represents the probability of accessing partition $ p $ given query $ q $, and $ c_t $ is the transfer cost. This approach achieves optimal performance when prefetching 2–3 partitions per query phase.</p>
<h3>3.3 Distributed Search Protocol</h3>

<p>Our implementation employs a two-phase search strategy: an initial parallel exploration phase across predicted relevant partitions, followed by a focused refinement phase that retrieves additional partitions only when necessary to maintain recall guarantees. This protocol reduces query latency by up to 47% compared to sequential access while maintaining 0.9 recall. Analysis shows that optimal bridge vertex selection can reduce cross-partition traversals by up to 64% while maintaining target recall levels.</p>
<h2>4. Performance Analysis</h2>

<p>We conducted extensive empirical evaluation using three standard datasets: Amazon-Catalog-1B (256-dimensional, 1 billion vectors), Amazon-Catalog-100M (256-dimensional, 100 million vectors), and GIST-1M (960-dimensional, 1 million vectors). Our analysis examines query latency, recall accuracy, and system scalability.</p>

<figure style="max-width:600px; margin: 1em auto; text-align:center;">
    <img src="images/s3/recall.png" alt="Recall Performance Analysis" style="width:100%; height:auto;">
    <figcaption>Figure 3: Recall performance across different datasets at various recall thresholds.</figcaption>
</figure>

<table style="border-collapse: collapse; width:100%; margin: 1em 0;">
    <thead>
        <tr style="border-bottom: 2px solid #333;">
            <th style="text-align:left; padding: 0.5em;">Dataset</th>
            <th style="text-align:left; padding: 0.5em;">Dimensions &amp; Size</th>
            <th style="text-align:left; padding: 0.5em;">Recall &gt; 0.9</th>
            <th style="text-align:left; padding: 0.5em;">0.8 &lt; Recall &lt; 0.9</th>
        </tr>
    </thead>
    <tbody>
        <tr style="border-bottom: 1px solid #ccc;">
            <td style="padding: 0.5em;">Amazon Catalog</td>
            <td style="padding: 0.5em;">256-d, 1B vectors</td>
            <td style="padding: 0.5em;">1.78s</td>
            <td style="padding: 0.5em;">1.27s</td>
        </tr>
        <tr style="border-bottom: 1px solid #ccc;">
            <td style="padding: 0.5em;">Amazon Catalog</td>
            <td style="padding: 0.5em;">256-d, 100M vectors</td>
            <td style="padding: 0.5em;">1.44s</td>
            <td style="padding: 0.5em;">0.90s</td>
        </tr>
        <tr>
            <td style="padding: 0.5em;">GIST Benchmark</td>
            <td style="padding: 0.5em;">960-d, 1M vectors</td>
            <td style="padding: 0.5em;">0.53s</td>
            <td style="padding: 0.5em;">0.39s</td>
        </tr>
    </tbody>
</table>

<p>These results reveal a consistent trend: as dataset size and dimensionality increase, achieving higher recall slightly elevates query latency. However, the incremental cost for maintaining recall above 0.9 remains moderate, especially for smaller datasets like GIST. The Amazon-Catalog datasets demonstrate that even at billion-scale, balancing parallel retrievals and partition sizing allows the system to sustain high recall with manageable latency increases.</p>
<h3>4.1 Query Latency Analysis</h3>

<div class="mermaid">
graph LR
    subgraph "Query Pipeline Timing"
        A[Cluster Identification: 2.2%] --> B[Process Pool Init: 39.3%]
        B --> C[Parallel S3 Fetch: 53.4%]
        C --> D[Post-processing: 5.1%]
    end
    
style A fill:#f9f,stroke:#333
style B fill:#bbf,stroke:#333
style C fill:#bfb,stroke:#333
style D fill:#fdd,stroke:#333
</div>

<p>Figure 4: Query pipeline timing breakdown showing the dominance of I/O operations (92.7% total) in the overall query latency.</p>

<h3>4.2 Performance Characteristics</h3>

<p>Our analysis reveals several key performance relationships in the system. Partition size significantly impacts system behavior, with sizes between 64MB and 128MB achieving optimal performance at 0.9 recall. Beyond 128MB, increased data transfer costs outweigh the benefits of reduced partition accesses.</p>

<p>The relationship between partition size and system performance shows three distinct patterns:</p>

<div class="equation">
    $$\begin{align*}
    \text{Query Latency} &\propto n \cdot \text{size} \quad \text{(linear)}\\
    \text{Data Transfer} &\propto \text{size}^{1+\epsilon} \quad \text{(superlinear)}\\
    \text{Leaf Accesses} &\propto \log(\text{size}) \quad \text{(logarithmic)}
    \end{align*}$$
</div>

<p>The trade-off between recall and resource utilization follows a characteristic pattern, with an elbow point at recall ≈ 0.875:</p>

<div class="equation">
    $$R(I) = g(s) + \beta \log(n)$$
</div>

<p>where $s$ represents the structure size, $n$ is the partition count, and $\beta$ is a scaling parameter.</p>


<h3>4.3 Scalability Analysis</h3>

<p>The system demonstrates strong scalability characteristics across dataset sizes from 1M to 1B vectors. Query latency scales sub-linearly, increasing by only 3.36x for a 10x increase in data volume, while memory overhead grows logarithmically with dataset size. Parallel retrieval efficiency improves with scale, showing better amortization of initialization costs. Moving from 100M to 1B vectors results in only a 2.2x latency increase while maintaining 0.9 recall, demonstrating the effectiveness of our parallel retrieval strategy.</p>
<h2>5. Research Contributions and Future Directions</h2>

<p>Our research establishes fundamental contributions to distributed graph search systems and cloud-native vector search architectures, with particular focus on the theoretical foundations of graph partitioning in distributed environments.</p>

<h3>5.1 Theoretical Framework</h3>
<p>We introduce the concept of "partition connectivity preservation" (PCP), which quantifies the relationship between cross-partition edges and search quality:</p>

<div class="equation">
    $$P(G) = \sum_{i=1}^{n} \frac{w_i c_i}{|E|}$$
</div>

<p>where $w_i$ represents the importance of preserved path $i$, $c_i$ is the connectivity measure, and $|E|$ is the number of cross-partition edges. For optimal bridge vertex selection, we define:</p>

<div class="equation">
    $$B(v) = d(v) \cdot c(v) \cdot u(v)$$
</div>

<p>where $d(v)$ is the vertex degree, $c(v)$ is the centrality measure, and $u(v)$ represents the cross-partition utility.</p>

<div class="mermaid">
graph TD
    subgraph "Graph Partitioning Properties"
        A[Original Graph] --> B[Partition Boundary Detection]
        B --> C[Bridge Node Selection]
        C --> D[Path Preservation]
        
        E[Local Connectivity] --> F[Global Reachability]
        F --> G[Search Quality]
    end

style A fill:#f9f,stroke:#333
style E fill:#bbf,stroke:#333
</div>

<p>Figure 5: Core components of the partitioning framework showing the relationship between local connectivity and global search quality.</p>

<h3>5.2 Key Findings</h3>

<p>Our research reveals two fundamental properties of partitioned search graphs. First, partition effectiveness follows a power-law distribution with respect to size, with exponential decay in search quality beyond critical boundaries. This suggests an inherent limit to partition size optimization governed by graph structure rather than storage constraints. Second, maintaining search quality requires preserving k-hop neighborhoods around partition boundaries, with diminishing returns beyond k=3.</p>

<h3>5.3 Future Directions and Implications</h3>

<p>Three promising research directions emerge from our findings. Dynamic partition adaptation could enable real-time response to query patterns, with initial experiments showing potential 35% efficiency improvements. Investigation of theoretical bounds on cross-partition connectivity could establish fundamental limits for recall guarantees. Multi-modal graph partitioning presents opportunities for topology-aware partitioning strategies, particularly effective for heterogeneous graph structures.</p>

<p>Beyond vector search, our findings have broader implications for distributed graph systems. The demonstrated need to jointly optimize local structure and global access patterns challenges traditional approaches that treat these as separate concerns. This insight opens new avenues for research in distributed graph algorithms, from social network analysis to molecular graph search and distributed neural networks.</p>

</body>
</html>
